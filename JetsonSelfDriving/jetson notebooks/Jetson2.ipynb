{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "textile-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-equation",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "knowing-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #image = torch.from_numpy(np.load(img_path + \".npy\").reshape(3,224,224))\n",
    "        image = read_image(img_path)\n",
    "        label = torch.Tensor([self.img_labels.iloc[idx, 1].astype(np.float32)])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001       \n",
    "    \n",
    "dataset = CustomImageDataset('data - Copy.csv', 'lineImages/')    \n",
    "    \n",
    "dataset = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-turning",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "about-feedback",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,9,(3,3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(9,18,(3,3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(18,36,(3,3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(36,64,(3,3), padding=1)\n",
    "        self.conv5 = nn.Conv2d(64,128,(3,3), padding=1)\n",
    "        self.fc1 = nn.Linear(100352,256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, test):\n",
    "#         if test == False:\n",
    "#             plt.imshow(x[0].permute(1,2,0))\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             plt.imshow(x.permute(1,2,0))\n",
    "#             plt.show()\n",
    "            \n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        \n",
    "        if test == False:\n",
    "            x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        else:\n",
    "            x = torch.flatten(x)\n",
    "        #x = torch.flatten(x)    \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "owned-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleImage(img):\n",
    "    q = img.numpy()\n",
    "    q = q/255\n",
    "    q = torch.from_numpy(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-hazard",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structured-toolbox",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Tuple:     1] loss: 0.013\n",
      "[Epoch: 2, Tuple:     1] loss: 0.013\n",
      "[Epoch: 3, Tuple:     1] loss: 0.015\n",
      "[Epoch: 4, Tuple:     1] loss: 0.011\n",
      "[Epoch: 5, Tuple:     1] loss: 0.014\n",
      "[Epoch: 6, Tuple:     1] loss: 0.014\n",
      "[Epoch: 7, Tuple:     1] loss: 0.013\n",
      "[Epoch: 8, Tuple:     1] loss: 0.017\n",
      "[Epoch: 9, Tuple:     1] loss: 0.011\n",
      "[Epoch: 10, Tuple:     1] loss: 0.014\n",
      "[Epoch: 11, Tuple:     1] loss: 0.010\n",
      "[Epoch: 12, Tuple:     1] loss: 0.016\n",
      "[Epoch: 13, Tuple:     1] loss: 0.010\n",
      "[Epoch: 14, Tuple:     1] loss: 0.010\n",
      "[Epoch: 15, Tuple:     1] loss: 0.009\n",
      "[Epoch: 16, Tuple:     1] loss: 0.006\n",
      "[Epoch: 17, Tuple:     1] loss: 0.005\n",
      "[Epoch: 18, Tuple:     1] loss: 0.008\n",
      "[Epoch: 19, Tuple:     1] loss: 0.005\n",
      "[Epoch: 20, Tuple:     1] loss: 0.006\n",
      "[Epoch: 21, Tuple:     1] loss: 0.004\n",
      "[Epoch: 22, Tuple:     1] loss: 0.003\n",
      "[Epoch: 23, Tuple:     1] loss: 0.004\n",
      "[Epoch: 24, Tuple:     1] loss: 0.003\n",
      "[Epoch: 25, Tuple:     1] loss: 0.004\n",
      "[Epoch: 26, Tuple:     1] loss: 0.001\n",
      "[Epoch: 27, Tuple:     1] loss: 0.002\n",
      "[Epoch: 28, Tuple:     1] loss: 0.003\n",
      "[Epoch: 29, Tuple:     1] loss: 0.002\n",
      "[Epoch: 30, Tuple:     1] loss: 0.002\n",
      "[Epoch: 31, Tuple:     1] loss: 0.001\n",
      "[Epoch: 32, Tuple:     1] loss: 0.001\n",
      "[Epoch: 33, Tuple:     1] loss: 0.001\n",
      "[Epoch: 34, Tuple:     1] loss: 0.002\n",
      "[Epoch: 35, Tuple:     1] loss: 0.001\n",
      "[Epoch: 36, Tuple:     1] loss: 0.001\n",
      "[Epoch: 37, Tuple:     1] loss: 0.001\n",
      "[Epoch: 38, Tuple:     1] loss: 0.001\n",
      "[Epoch: 39, Tuple:     1] loss: 0.001\n",
      "[Epoch: 40, Tuple:     1] loss: 0.001\n",
      "[Epoch: 41, Tuple:     1] loss: 0.001\n",
      "[Epoch: 42, Tuple:     1] loss: 0.000\n",
      "[Epoch: 43, Tuple:     1] loss: 0.001\n",
      "[Epoch: 44, Tuple:     1] loss: 0.000\n",
      "[Epoch: 45, Tuple:     1] loss: 0.000\n",
      "[Epoch: 46, Tuple:     1] loss: 0.000\n",
      "[Epoch: 47, Tuple:     1] loss: 0.000\n",
      "[Epoch: 48, Tuple:     1] loss: 0.001\n",
      "[Epoch: 49, Tuple:     1] loss: 0.000\n",
      "[Epoch: 50, Tuple:     1] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (images,labels) in enumerate(dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images = scaleImage(images)\n",
    "        #labels = scaleLabel(labels)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = net(images.float(), False)\n",
    "        \n",
    "        labels = labels.unsqueeze(1)\n",
    "        #print(str(labels) + \" \" + str(outputs))\n",
    "        loss = criterion(outputs.unsqueeze(1), labels)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 10 mini-batches\n",
    "            print(f'[Epoch: {epoch + 1}, Tuple: {i + 1:5d}] loss: {running_loss / 9:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lucky-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './jetson50.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Loading the trained network\n",
    "#net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heated-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Prediction:\n",
      "[0.], [0.00374356]\n",
      "[0.], [-0.0207608]\n",
      "[0.], [-0.03665822]\n",
      "[0.], [0.03407381]\n",
      "[0.5], [0.54775655]\n",
      "[-0.25], [-0.23333643]\n",
      "[-0.25], [-0.2919544]\n",
      "[-0.25], [-0.2700745]\n",
      "[0.], [0.00547]\n",
      "[0.], [0.00572329]\n",
      "[0.1], [0.15498313]\n",
      "[0.], [-0.00269682]\n",
      "[0.05], [0.07751779]\n",
      "[-0.1], [-0.10059048]\n",
      "[-0.5], [-0.52059096]\n",
      "[-0.05], [-0.0747688]\n",
      "[0.5], [0.4947714]\n",
      "[0.25], [0.29362497]\n",
      "[-0.1], [-0.10046734]\n",
      "[-0.05], [-0.05026007]\n",
      "[0.], [0.02818231]\n",
      "[0.025], [0.1110718]\n",
      "[-0.5], [-0.82914406]\n",
      "[0.1], [0.14294569]\n",
      "[-0.05], [-0.0769152]\n",
      "[-0.7], [-0.6771695]\n",
      "[0.], [0.06072842]\n",
      "[0.], [0.03430657]\n",
      "[0.5], [0.5539761]\n",
      "[1.], [0.96105725]\n",
      "[1.], [0.87164843]\n",
      "[1.], [0.9536912]\n",
      "[0.5], [0.5447535]\n",
      "[-1.], [-0.8953686]\n",
      "[0.025], [0.02438519]\n",
      "[-0.25], [-0.23602971]\n",
      "[-0.3], [-0.3217786]\n",
      "[0.05], [0.0621318]\n",
      "[-0.5], [-0.63017875]\n",
      "[0.05], [0.06007835]\n",
      "[0.05], [0.15766867]\n",
      "[0.025], [0.01219025]\n",
      "[0.2], [0.25183678]\n",
      "[0.], [0.01672929]\n",
      "[0.7], [0.7178586]\n",
      "[-0.5], [-0.4698207]\n",
      "[0.], [-0.00836034]\n",
      "[-0.025], [-0.02199209]\n",
      "[-0.1], [-0.12381066]\n",
      "[-0.], [-0.02254978]\n",
      "[0.025], [0.12943]\n",
      "[-0.1], [-0.10196891]\n",
      "[0.], [0.03698612]\n",
      "[-0.2], [-0.20461169]\n",
      "[-0.5], [-0.46961132]\n",
      "[0.], [0.01063568]\n",
      "[0.1], [0.14155018]\n",
      "[-0.1], [-0.13450614]\n",
      "[0.2], [0.27067596]\n",
      "[0.5], [0.5542712]\n",
      "[0.], [0.03106969]\n",
      "[0.2], [0.20707788]\n",
      "[-0.5], [-0.6976867]\n",
      "[0.4], [0.41668755]\n",
      "[0.2], [0.20004982]\n",
      "[0.2], [0.24572419]\n",
      "[-0.3], [-0.29611987]\n",
      "[0.], [0.02911134]\n",
      "[-0.5], [-0.39989436]\n",
      "[0.1], [0.14404455]\n",
      "[0.], [-0.00193781]\n",
      "[0.3], [0.3478146]\n",
      "[0.2], [0.21930811]\n",
      "[0.2], [0.21659407]\n",
      "[0.4], [0.4620545]\n",
      "[-1.], [-0.82922995]\n",
      "[0.2], [0.21047312]\n",
      "[0.], [0.05590062]\n",
      "[0.1], [0.1785544]\n",
      "[0.1], [0.15898146]\n",
      "[0.3], [0.38499695]\n",
      "[0.], [0.00075161]\n",
      "[0.1], [0.08177477]\n",
      "[0.8], [0.7436608]\n",
      "[0.], [0.00096953]\n",
      "[0.3], [0.2900488]\n",
      "[0.5], [0.4949929]\n",
      "[0.], [0.08173567]\n",
      "[-0.05], [0.00023529]\n",
      "[-0.1], [-0.10368657]\n",
      "[-0.1], [-0.13211727]\n",
      "[0.9], [0.8777245]\n",
      "[-0.25], [-0.2434561]\n",
      "[-0.9], [-0.87208706]\n",
      "[-1.], [-0.918951]\n",
      "[-0.8], [-0.8106311]\n",
      "[0.3], [0.32056716]\n",
      "[0.3], [0.33359084]\n",
      "[0.4], [0.40513432]\n",
      "[0.3], [0.31656078]\n",
      "[-0.3], [-0.31002584]\n",
      "[0.9], [0.892491]\n",
      "[0.], [-0.01722384]\n",
      "[0.], [0.00955418]\n",
      "[0.], [0.09665996]\n",
      "[-0.25], [-0.1959813]\n",
      "[0.3], [0.33304515]\n",
      "[0.2], [0.26167288]\n",
      "[0.5], [0.5399021]\n",
      "[0.], [0.06554533]\n",
      "[0.], [0.04406977]\n",
      "[0.2], [0.24488667]\n",
      "[0.], [0.00869999]\n",
      "[0.], [0.00929549]\n",
      "[-0.1], [-0.11750863]\n",
      "[-0.3], [-0.25041765]\n",
      "[0.], [0.00234222]\n",
      "[0.1], [0.12305905]\n",
      "[0.1], [0.23936187]\n",
      "[0.1], [0.20787214]\n",
      "[-0.025], [-0.02624693]\n",
      "[-0.2], [-0.20444994]\n",
      "[0.3], [0.37786114]\n",
      "[0.3], [0.36269885]\n",
      "[-0.4], [-0.433152]\n",
      "[-0.05], [-0.02266692]\n",
      "[-0.05], [-0.05974612]\n",
      "[0.05], [0.06498373]\n",
      "[0.1], [0.11668099]\n",
      "[0.2], [0.20596007]\n",
      "[-0.1], [-0.09005949]\n",
      "[0.2], [0.2738107]\n",
      "[0.3], [0.34968215]\n",
      "[0.5], [0.39139965]\n",
      "[0.3], [0.2730147]\n",
      "[0.5], [0.50422484]\n",
      "[0.3], [0.25863257]\n",
      "[-0.1], [-0.07913048]\n",
      "[-0.2], [-0.17638691]\n",
      "[0.4], [0.47873726]\n",
      "[0.5], [0.56559175]\n",
      "[-0.2], [-0.17681569]\n",
      "[-0.7], [-0.7398047]\n",
      "[0.4], [0.42224976]\n",
      "[0.2], [0.21251738]\n",
      "[0.1], [0.16313969]\n",
      "[-0.1], [-0.09353143]\n",
      "[-0.1], [-0.13368328]\n",
      "[0.3], [0.32527024]\n",
      "[-0.1], [-0.12199225]\n",
      "[0.4], [0.4044661]\n",
      "[-0.1], [-0.1342669]\n",
      "[-0.2], [-0.20406315]\n",
      "[-0.1], [-0.08919173]\n",
      "[0.], [0.03567882]\n",
      "[-0.8], [-0.8757704]\n",
      "[0.], [-0.00081871]\n",
      "[0.3], [0.35730854]\n",
      "[-0.8], [-0.79481536]\n",
      "[0.2], [0.24569322]\n",
      "[0.2], [0.18661977]\n",
      "[0.1], [0.10681168]\n",
      "[-0.1], [-0.10871843]\n",
      "[0.3], [0.3065923]\n",
      "[0.1], [0.10069672]\n",
      "[-0.1], [-0.11298777]\n",
      "[0.1], [0.2524367]\n",
      "[-0.1], [-0.06360696]\n",
      "[1.], [0.9607221]\n",
      "[-0.5], [-0.5180107]\n",
      "[0.5], [0.55972236]\n",
      "[-0.4], [-0.3339478]\n",
      "[-0.5], [-0.5162566]\n",
      "[0.3], [0.30920526]\n",
      "[0.1], [0.07382214]\n",
      "[0.05], [0.02527778]\n",
      "[-0.05], [-0.04756041]\n",
      "[0.1], [0.10884983]\n",
      "[0.3], [0.37814417]\n",
      "[0.2], [0.21196929]\n",
      "[-0.4], [-0.39789677]\n",
      "[-0.5], [-0.508462]\n",
      "[0.3], [0.28887373]\n",
      "[-0.2], [-0.1912751]\n",
      "[1.], [0.95754397]\n",
      "[0.3], [0.3430068]\n",
      "[0.3], [0.33486632]\n",
      "[0.2], [0.21569353]\n",
      "[0.1], [0.08798119]\n",
      "[0.], [0.03110371]\n",
      "[-0.1], [-0.14377275]\n",
      "[-0.3], [-0.33226386]\n",
      "[-0.4], [-0.40477514]\n",
      "[0.1], [0.09207885]\n",
      "[-0.3], [-0.29990646]\n",
      "[0.1], [0.13235748]\n",
      "[-0.2], [-0.18444313]\n",
      "[-0.025], [0.00646161]\n",
      "[-0.4], [-0.36385992]\n",
      "[0.5], [0.5894735]\n",
      "[0.3], [0.3036492]\n",
      "[-0.1], [-0.07703673]\n",
      "[0.1], [0.13985647]\n",
      "[0.6], [0.65691274]\n",
      "[0.3], [0.30647472]\n",
      "[-0.25], [-0.23815268]\n",
      "[-0.6], [-0.58438855]\n",
      "[0.3], [0.3066614]\n",
      "[0.025], [0.0510759]\n",
      "[0.], [0.0044467]\n",
      "[0.025], [0.0615255]\n",
      "[-0.025], [0.07188866]\n",
      "[0.025], [0.08268408]\n",
      "[1.], [0.93156105]\n",
      "[-0.4], [-0.39652348]\n",
      "[0.2], [0.26033276]\n",
      "[0.1], [0.13816239]\n",
      "[-0.02], [-0.02296172]\n",
      "[0.025], [0.04535294]\n",
      "[1.], [0.9434643]\n",
      "[0.3], [0.33566386]\n",
      "[0.4], [0.43177944]\n",
      "[0.5], [0.53365314]\n",
      "[-0.6], [-0.66429144]\n",
      "[-1.], [-0.82717395]\n",
      "[0.5], [0.54808027]\n",
      "[0.5], [0.5763047]\n",
      "[-0.1], [-0.0813774]\n",
      "[0.1], [0.13630126]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "print(\"Actual: Prediction:\")\n",
    "dataset = CustomImageDataset('data - Copy.csv', 'lineImages/')    \n",
    "#net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in dataset:\n",
    "        images, labels = data\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        images = scaleImage(images)\n",
    "        outputs = net(images.float(), True)\n",
    "        predicted = outputs.data\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.data == labels).sum().item()\n",
    "        print(str(np.asarray(labels)) + \", \" + str(np.asarray(outputs.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-restoration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
