{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08345316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab41c9e",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f9d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #image = torch.from_numpy(np.load(img_path + \".npy\").reshape(3,224,224))\n",
    "        image = read_image(img_path)\n",
    "        label = torch.Tensor([self.img_labels.iloc[idx, 1].astype(np.float32)])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001       \n",
    "    \n",
    "dataset = CustomImageDataset('data - Copy.csv', 'lineImages/')    \n",
    "    \n",
    "dataset = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b8441",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21bc549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,9,(3,3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(9,18,(3,3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(18,36,(3,3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(36,64,(3,3), padding=1)\n",
    "        self.conv5 = nn.Conv2d(64,128,(3,3), padding=1)\n",
    "        self.fc1 = nn.Linear(100352,256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, test):\n",
    "#         if test == False:\n",
    "#             plt.imshow(x[0].permute(1,2,0))\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             plt.imshow(x.permute(1,2,0))\n",
    "#             plt.show()\n",
    "            \n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        \n",
    "        if test == False:\n",
    "            x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        else:\n",
    "            x = torch.flatten(x)\n",
    "        #x = torch.flatten(x)    \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee68839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleImage(img):\n",
    "    q = img.numpy()\n",
    "    q = q/255\n",
    "    q = torch.from_numpy(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45b87b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b550a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Tuple:     1] loss: 0.016\n",
      "[Epoch: 2, Tuple:     1] loss: 0.014\n",
      "[Epoch: 3, Tuple:     1] loss: 0.007\n",
      "[Epoch: 4, Tuple:     1] loss: 0.021\n",
      "[Epoch: 5, Tuple:     1] loss: 0.014\n",
      "[Epoch: 6, Tuple:     1] loss: 0.014\n",
      "[Epoch: 7, Tuple:     1] loss: 0.019\n",
      "[Epoch: 8, Tuple:     1] loss: 0.008\n",
      "[Epoch: 9, Tuple:     1] loss: 0.010\n",
      "[Epoch: 10, Tuple:     1] loss: 0.011\n",
      "[Epoch: 11, Tuple:     1] loss: 0.008\n",
      "[Epoch: 12, Tuple:     1] loss: 0.014\n",
      "[Epoch: 13, Tuple:     1] loss: 0.008\n",
      "[Epoch: 14, Tuple:     1] loss: 0.012\n",
      "[Epoch: 15, Tuple:     1] loss: 0.008\n",
      "[Epoch: 16, Tuple:     1] loss: 0.009\n",
      "[Epoch: 17, Tuple:     1] loss: 0.007\n",
      "[Epoch: 18, Tuple:     1] loss: 0.009\n",
      "[Epoch: 19, Tuple:     1] loss: 0.005\n",
      "[Epoch: 20, Tuple:     1] loss: 0.003\n",
      "[Epoch: 21, Tuple:     1] loss: 0.002\n",
      "[Epoch: 22, Tuple:     1] loss: 0.002\n",
      "[Epoch: 23, Tuple:     1] loss: 0.001\n",
      "[Epoch: 24, Tuple:     1] loss: 0.004\n",
      "[Epoch: 25, Tuple:     1] loss: 0.002\n",
      "[Epoch: 26, Tuple:     1] loss: 0.003\n",
      "[Epoch: 27, Tuple:     1] loss: 0.002\n",
      "[Epoch: 28, Tuple:     1] loss: 0.002\n",
      "[Epoch: 29, Tuple:     1] loss: 0.002\n",
      "[Epoch: 30, Tuple:     1] loss: 0.001\n",
      "[Epoch: 31, Tuple:     1] loss: 0.001\n",
      "[Epoch: 32, Tuple:     1] loss: 0.001\n",
      "[Epoch: 33, Tuple:     1] loss: 0.001\n",
      "[Epoch: 34, Tuple:     1] loss: 0.001\n",
      "[Epoch: 35, Tuple:     1] loss: 0.001\n",
      "[Epoch: 36, Tuple:     1] loss: 0.001\n",
      "[Epoch: 37, Tuple:     1] loss: 0.000\n",
      "[Epoch: 38, Tuple:     1] loss: 0.001\n",
      "[Epoch: 39, Tuple:     1] loss: 0.001\n",
      "[Epoch: 40, Tuple:     1] loss: 0.000\n",
      "[Epoch: 41, Tuple:     1] loss: 0.000\n",
      "[Epoch: 42, Tuple:     1] loss: 0.000\n",
      "[Epoch: 43, Tuple:     1] loss: 0.000\n",
      "[Epoch: 44, Tuple:     1] loss: 0.000\n",
      "[Epoch: 45, Tuple:     1] loss: 0.001\n",
      "[Epoch: 46, Tuple:     1] loss: 0.000\n",
      "[Epoch: 47, Tuple:     1] loss: 0.001\n",
      "[Epoch: 48, Tuple:     1] loss: 0.000\n",
      "[Epoch: 49, Tuple:     1] loss: 0.000\n",
      "[Epoch: 50, Tuple:     1] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (images,labels) in enumerate(dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images = scaleImage(images)\n",
    "        #labels = scaleLabel(labels)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = net(images.float(), False)\n",
    "        \n",
    "        labels = labels.unsqueeze(1)\n",
    "        #print(str(labels) + \" \" + str(outputs))\n",
    "        loss = criterion(outputs.unsqueeze(1), labels)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 10 mini-batches\n",
    "            print(f'[Epoch: {epoch + 1}, Tuple: {i + 1:5d}] loss: {running_loss / 9:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7277361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './jetson.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Loading the trained network\n",
    "#net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f33102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Prediction:\n",
      "[0.], [-0.01855229]\n",
      "[0.], [-0.02725376]\n",
      "[0.], [-0.09067733]\n",
      "[0.], [-0.00132646]\n",
      "[0.5], [0.64229125]\n",
      "[-0.25], [-0.23426959]\n",
      "[-0.25], [-0.27955362]\n",
      "[-0.25], [-0.3192867]\n",
      "[0.], [-0.04313525]\n",
      "[0.], [-0.04068213]\n",
      "[0.1], [0.20859954]\n",
      "[0.], [-0.01031652]\n",
      "[0.05], [0.04648495]\n",
      "[-0.1], [-0.1177081]\n",
      "[-0.5], [-0.51397645]\n",
      "[-0.05], [-0.14761779]\n",
      "[0.5], [0.58812016]\n",
      "[0.25], [0.27976763]\n",
      "[-0.1], [-0.16445331]\n",
      "[-0.05], [-0.10444861]\n",
      "[0.], [-0.00330352]\n",
      "[0.025], [0.0927512]\n",
      "[-0.5], [-0.7433291]\n",
      "[0.1], [0.06320482]\n",
      "[-0.05], [-0.07102908]\n",
      "[-0.7], [-0.7018535]\n",
      "[0.], [-0.00063344]\n",
      "[0.], [0.02441997]\n",
      "[0.5], [0.5602082]\n",
      "[1.], [0.9550242]\n",
      "[1.], [0.8151367]\n",
      "[1.], [0.9428721]\n",
      "[0.5], [0.61454546]\n",
      "[-1.], [-0.87560445]\n",
      "[0.025], [0.04964324]\n",
      "[-0.25], [-0.22147986]\n",
      "[-0.3], [-0.41009998]\n",
      "[0.05], [0.09789502]\n",
      "[-0.5], [-0.6282585]\n",
      "[0.05], [0.04076931]\n",
      "[0.05], [0.13907929]\n",
      "[0.025], [0.01123299]\n",
      "[0.2], [0.22114508]\n",
      "[0.], [-0.00284143]\n",
      "[0.7], [0.6779291]\n",
      "[-0.5], [-0.50869733]\n",
      "[0.], [-0.02458373]\n",
      "[-0.025], [-0.0231523]\n",
      "[-0.1], [-0.14514153]\n",
      "[-0.], [-0.04545698]\n",
      "[0.025], [0.16733676]\n",
      "[-0.1], [-0.14891526]\n",
      "[0.], [-0.0340816]\n",
      "[-0.2], [-0.23391166]\n",
      "[-0.5], [-0.5348532]\n",
      "[0.], [-0.01907377]\n",
      "[0.1], [0.16041893]\n",
      "[-0.1], [-0.19858524]\n",
      "[0.2], [0.272029]\n",
      "[0.5], [0.52802384]\n",
      "[0.], [-0.01348079]\n",
      "[0.2], [0.05229456]\n",
      "[-0.5], [-0.6111435]\n",
      "[0.4], [0.51544845]\n",
      "[0.2], [0.17693093]\n",
      "[0.2], [0.3668372]\n",
      "[-0.3], [-0.2829011]\n",
      "[0.], [-0.02637888]\n",
      "[-0.5], [-0.44460523]\n",
      "[0.1], [0.09790238]\n",
      "[0.], [-0.02811863]\n",
      "[0.3], [0.35260653]\n",
      "[0.2], [0.18464553]\n",
      "[0.2], [0.14306779]\n",
      "[0.4], [0.42633313]\n",
      "[-1.], [-0.8317135]\n",
      "[0.2], [0.23576437]\n",
      "[0.], [0.00950061]\n",
      "[0.1], [0.17053376]\n",
      "[0.1], [-0.02873385]\n",
      "[0.3], [0.41091797]\n",
      "[0.], [-0.04398583]\n",
      "[0.1], [0.12506731]\n",
      "[0.8], [0.7531528]\n",
      "[0.], [-0.05080977]\n",
      "[0.3], [0.36184514]\n",
      "[0.5], [0.6075764]\n",
      "[0.], [0.04977595]\n",
      "[-0.05], [-0.00045715]\n",
      "[-0.1], [-0.15727289]\n",
      "[-0.1], [-0.14510857]\n",
      "[0.9], [0.8554884]\n",
      "[-0.25], [-0.2863309]\n",
      "[-0.9], [-0.8680133]\n",
      "[-1.], [-0.91051567]\n",
      "[-0.8], [-0.7849031]\n",
      "[0.3], [0.38320166]\n",
      "[0.3], [0.3196563]\n",
      "[0.4], [0.37997845]\n",
      "[0.3], [0.32626054]\n",
      "[-0.3], [-0.33456263]\n",
      "[0.9], [0.90834683]\n",
      "[0.], [-0.1005861]\n",
      "[0.], [-6.4074993e-07]\n",
      "[0.], [0.08703288]\n",
      "[-0.25], [-0.20951296]\n",
      "[0.3], [0.32298347]\n",
      "[0.2], [0.18464316]\n",
      "[0.5], [0.51143503]\n",
      "[0.], [0.08523994]\n",
      "[0.], [0.11773243]\n",
      "[0.2], [0.26181617]\n",
      "[0.], [0.04513589]\n",
      "[0.], [-0.010161]\n",
      "[-0.1], [-0.14577556]\n",
      "[-0.3], [-0.22514476]\n",
      "[0.], [-0.04409306]\n",
      "[0.1], [0.0312942]\n",
      "[0.1], [0.25401223]\n",
      "[0.1], [0.20445706]\n",
      "[-0.025], [-0.05896027]\n",
      "[-0.2], [-0.24492869]\n",
      "[0.3], [0.3491102]\n",
      "[0.3], [0.3343089]\n",
      "[-0.4], [-0.42003217]\n",
      "[-0.05], [-0.03485217]\n",
      "[-0.05], [-0.07261018]\n",
      "[0.05], [0.10111339]\n",
      "[0.1], [0.16877416]\n",
      "[0.2], [0.21677093]\n",
      "[-0.1], [-0.10313158]\n",
      "[0.2], [0.2718904]\n",
      "[0.3], [0.34302828]\n",
      "[0.5], [0.34067088]\n",
      "[0.3], [0.26244667]\n",
      "[0.5], [0.5323453]\n",
      "[0.3], [0.25210592]\n",
      "[-0.1], [-0.14008273]\n",
      "[-0.2], [-0.1873827]\n",
      "[0.4], [0.48573777]\n",
      "[0.5], [0.58076024]\n",
      "[-0.2], [-0.23818156]\n",
      "[-0.7], [-0.74582475]\n",
      "[0.4], [0.47380978]\n",
      "[0.2], [0.19469593]\n",
      "[0.1], [0.20071805]\n",
      "[-0.1], [-0.16944502]\n",
      "[-0.1], [-0.10556746]\n",
      "[0.3], [0.3558553]\n",
      "[-0.1], [-0.11208604]\n",
      "[0.4], [0.3938111]\n",
      "[-0.1], [-0.14900091]\n",
      "[-0.2], [-0.23645681]\n",
      "[-0.1], [-0.09221968]\n",
      "[0.], [0.00890716]\n",
      "[-0.8], [-0.861038]\n",
      "[0.], [-0.00132818]\n",
      "[0.3], [0.31236443]\n",
      "[-0.8], [-0.77731085]\n",
      "[0.2], [0.2319739]\n",
      "[0.2], [0.22624883]\n",
      "[0.1], [0.13954301]\n",
      "[-0.1], [-0.09857692]\n",
      "[0.3], [0.35146487]\n",
      "[0.1], [0.0645686]\n",
      "[-0.1], [-0.11765306]\n",
      "[0.1], [0.2092318]\n",
      "[-0.1], [-0.16149247]\n",
      "[1.], [0.9399978]\n",
      "[-0.5], [-0.49220884]\n",
      "[0.5], [0.42438385]\n",
      "[-0.4], [-0.36400872]\n",
      "[-0.5], [-0.5320669]\n",
      "[0.3], [0.27564934]\n",
      "[0.1], [0.10738788]\n",
      "[0.05], [0.07306357]\n",
      "[-0.05], [-0.05914053]\n",
      "[0.1], [0.09590575]\n",
      "[0.3], [0.40487215]\n",
      "[0.2], [0.2165785]\n",
      "[-0.4], [-0.49535266]\n",
      "[-0.5], [-0.513724]\n",
      "[0.3], [0.28364843]\n",
      "[-0.2], [-0.23293588]\n",
      "[1.], [0.9203059]\n",
      "[0.3], [0.25276467]\n",
      "[0.3], [0.37830094]\n",
      "[0.2], [0.25560126]\n",
      "[0.1], [0.09641122]\n",
      "[0.], [0.03596737]\n",
      "[-0.1], [-0.15639557]\n",
      "[-0.3], [-0.39056194]\n",
      "[-0.4], [-0.41301513]\n",
      "[0.1], [0.07517777]\n",
      "[-0.3], [-0.35179386]\n",
      "[0.1], [0.07784801]\n",
      "[-0.2], [-0.22261946]\n",
      "[-0.025], [-0.03642799]\n",
      "[-0.4], [-0.5130087]\n",
      "[0.5], [0.57132596]\n",
      "[0.3], [0.30304685]\n",
      "[-0.1], [-0.13167256]\n",
      "[0.1], [0.13007139]\n",
      "[0.6], [0.72660404]\n",
      "[0.3], [0.28895348]\n",
      "[-0.25], [-0.25037676]\n",
      "[-0.6], [-0.5906698]\n",
      "[0.3], [0.3381905]\n",
      "[0.025], [0.04112937]\n",
      "[0.], [-0.01628606]\n",
      "[0.025], [0.04294385]\n",
      "[-0.025], [0.05804011]\n",
      "[0.025], [0.10045911]\n",
      "[1.], [0.8951318]\n",
      "[-0.4], [-0.44863316]\n",
      "[0.2], [0.21769926]\n",
      "[0.1], [0.12339868]\n",
      "[-0.02], [-0.0807177]\n",
      "[0.025], [0.08888878]\n",
      "[1.], [0.938702]\n",
      "[0.3], [0.31825465]\n",
      "[0.4], [0.4032525]\n",
      "[0.5], [0.5584896]\n",
      "[-0.6], [-0.58802545]\n",
      "[-1.], [-0.8242939]\n",
      "[0.5], [0.5600303]\n",
      "[0.5], [0.62564313]\n",
      "[-0.1], [-0.11764716]\n",
      "[0.1], [0.14284815]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "print(\"Actual: Prediction:\")\n",
    "dataset = CustomImageDataset('data - Copy.csv', 'lineImages/')    \n",
    "#net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in dataset:\n",
    "        images, labels = data\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        images = scaleImage(images)\n",
    "        outputs = net(images.float(), True)\n",
    "        predicted = outputs.data\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.data == labels).sum().item()\n",
    "        print(str(np.asarray(labels)) + \", \" + str(np.asarray(outputs.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237fe4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
